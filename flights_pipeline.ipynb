{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlightRadar24.api import FlightRadar24API\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_api = FlightRadar24API()\n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zones_toDF():\n",
    "    zones = fr_api.get_zones()\n",
    "    df_zones = spark.createDataFrame(zones)\n",
    "    return df_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_zones = extract_zones_toDF()\n",
    "\n",
    "schema_zones = StructType([\n",
    "    StructField(\"Continent\", StringType(), True),\n",
    "    StructField(\"br_x\", StringType(), True),\n",
    "    StructField(\"br_y\", StringType(), True),\n",
    "    StructField(\"tl_x\", StringType(), True),\n",
    "    StructField(\"tl_y\", FloatType(), True),\n",
    "    StructField(\"Subzone1\", FloatType(), True),\n",
    "    StructField(\"br_x_1\", StringType(), True),\n",
    "    StructField(\"br_y_1\", StringType(), True),\n",
    "    StructField(\"tl_x_1\", StringType(), True),\n",
    "    StructField(\"tl_y_1\", FloatType(), True),\n",
    "    StructField(\"Subzone2\", FloatType(), True),\n",
    "    StructField(\"br_x_2\", StringType(), True),\n",
    "    StructField(\"br_y_2\", StringType(), True),\n",
    "    StructField(\"tl_x_2\", StringType(), True),\n",
    "    StructField(\"tl_y_2\", FloatType(), True),\n",
    "    ])\n",
    "\n",
    "zones = fr_api.get_zones()\n",
    "#df = spark.read.json('/test.json')\n",
    "#df = spark.read.option(\"multiline\", \"true\").json('/test.json')\n",
    "#df.printSchema()\n",
    "\n",
    "\n",
    "type(zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StructType can not accept object 'europe' in type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[1;32m      2\u001b[0m   StructField(\u001b[39m'\u001b[39m\u001b[39mContinent\u001b[39m\u001b[39m'\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m   \u001b[39m#StructField('Continent_tl_y', StringType(), True),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[39m#StructField('Continent_br_x', StringType(), True)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m   ])\n\u001b[1;32m     11\u001b[0m continents \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39meurope\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m  \u001b[39m'\u001b[39m\u001b[39mnorthamerica\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m  \u001b[39m'\u001b[39m\u001b[39msouthamerica\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m  \u001b[39m'\u001b[39m\u001b[39mmaldives\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m  \u001b[39m'\u001b[39m\u001b[39mnorthatlantic\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(continents, schema\u001b[39m=\u001b[39;49mschema)\u001b[39m#.toDF(columns:_*)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     23\u001b[0m \u001b[39m\"\"\"for x in zones:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m    continents.append(x)\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:628\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(data)\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferSchemaFromList(data, names\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:910\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(obj):\n\u001b[0;32m--> 910\u001b[0m     verify_func(obj)\n\u001b[1;32m    911\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:1722\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mverify\u001b[39m(obj: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1722\u001b[0m         verify_value(obj)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:1706\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1704\u001b[0m         verifier(d\u001b[39m.\u001b[39mget(f))\n\u001b[1;32m   1705\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1706\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   1707\u001b[0m         new_msg(\u001b[39m\"\u001b[39m\u001b[39mStructType can not accept object \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m in type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (obj, \u001b[39mtype\u001b[39m(obj)))\n\u001b[1;32m   1708\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: StructType can not accept object 'europe' in type <class 'str'>"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "  StructField('Continent', StringType(), True)\n",
    "  #StructField('Continent_tl_y', StringType(), True),\n",
    "  #StructField('Continent_tl_x', StringType(), True),\n",
    "  #StructField('Continent_br_y', StringType(), True),\n",
    "  #StructField('Continent_br_x', StringType(), True)\n",
    "  ])\n",
    "\n",
    "\n",
    "\n",
    "continents = ['europe',\n",
    " 'northamerica',\n",
    " 'southamerica',\n",
    " 'oceania',\n",
    " 'asia',\n",
    " 'africa',\n",
    " 'atlantic',\n",
    " 'maldives',\n",
    " 'northatlantic']\n",
    "\n",
    "df = spark.createDataFrame(continents, schema=schema)#.toDF(columns:_*)\n",
    "df.show()\n",
    "\"\"\"for x in zones:\n",
    "    continents.append(x)\"\"\"\n",
    "\"\"\"if 'subzones' in zones[x]:\n",
    "        print(zones[x]['subzones'])\n",
    "    df.withColumn(\"bonus_amount\", df.salary*0.3).show()\"\"\"\n",
    "#continents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_airlines_toDF():\n",
    "    airlines = fr_api.get_airlines()\n",
    "    df_airlines = spark.createDataFrame(airlines)\n",
    "    return df_airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonne 'alt' contient qq données comme \"-1\" qui sont catégorisées comme string et pas int.\n",
    "La colonne 'lon' et 'lan contient qq données catégorisées comme int et pas float comme la majorité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_airports(lst):\n",
    "    result = [dict( [a, int(x)] if a == \"alt\" else [a, float(x)] if a == 'lon' or a == 'lat' else [a, x] for a, x in b.items()) for b in lst]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_airports_toDF():\n",
    "    airports = fr_api.get_airports()\n",
    "    airports_typed_columns = convert_airports(airports)\n",
    "\n",
    "    schema_airports = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"iata\", StringType(), True),\n",
    "    StructField(\"icao\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"lat\", FloatType(), True),\n",
    "    StructField(\"lon\", FloatType(), True),\n",
    "    StructField(\"alt\", IntegerType(), True)\n",
    "    ])\n",
    "    df_airports = spark.createDataFrame(airports_typed_columns, schema=schema_airports)\n",
    "\n",
    "    return df_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_flights(lst):\n",
    "    result = [b.__dict__ for b in lst]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flights_toDF():\n",
    "   \n",
    "   flights = fr_api.get_flights()\n",
    "   flights = convert_flights(flights)\n",
    "   \n",
    "   schema = StructType([\n",
    "   StructField(\"id\", StringType(), True),\n",
    "   StructField(\"icao_24bit\", StringType(), True),\n",
    "   StructField(\"latitude\", FloatType(), True),\n",
    "   StructField(\"longitude\", FloatType(), True),\n",
    "   StructField(\"heading\", IntegerType(), True),\n",
    "   StructField(\"altitude\", IntegerType(), True),\n",
    "   StructField(\"ground_speed\", IntegerType(), True),\n",
    "   StructField(\"squawk\", StringType(), True),\n",
    "   StructField(\"aircraft_code\", StringType(), True),\n",
    "   StructField(\"registration\", StringType(), True),\n",
    "   StructField(\"time\", IntegerType(), True),\n",
    "   StructField(\"origin_airport_iata\", StringType(), True),\n",
    "   StructField(\"destination_airport_iata\", StringType(), True),\n",
    "   StructField(\"number\", StringType(), True),\n",
    "   StructField(\"airline_iata\", StringType(), True),\n",
    "   StructField(\"on_ground\", IntegerType(), True),\n",
    "   StructField(\"vertical_speed\", IntegerType(), True),\n",
    "   StructField(\"callsign\", StringType(), True),\n",
    "   StructField(\"airline_icao\", StringType(), True)\n",
    "   ])\n",
    "\n",
    "   flights = fr_api.get_flights()\n",
    "   df_flights = spark.createDataFrame(flights, schema=schema)\n",
    "   return df_flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = extract_flights_toDF()\n",
    "df_airports = extract_airports_toDF()\n",
    "df_airlines = extract_airlines_toDF()\n",
    "#df_zones = extract_zones_toDF()\n",
    "\n",
    "#df_flights.show(10)\n",
    "df_airports.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chercher pour anomalies sur df_airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_quality(df, columnName ):\n",
    "    vals = df.select(columnName).count()\n",
    "    distinct_vals = df.select(columnName).distinct().count()\n",
    "    null_vals = df.filter((df[columnName] == \"\") | (df[columnName] == \"N/A\") | (df[columnName] == None) ).count()\n",
    "    print(f\"Column {(columnName)} values : {(vals)}\")\n",
    "    print(f\"Column {(columnName)} distinct values : {(distinct_vals)}\")\n",
    "    print(f\"Column {(columnName)} null values : {(null_vals)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df_quality(df, df_name):\n",
    "    vals = df.count() \n",
    "    distinct_vals = df.distinct().count() \n",
    "    print(f\"Dataframe {(df_name)}  values : {(vals)}\")\n",
    "    print(f\"Dataframe {(df_name)} distinct values : {(distinct_vals)}\\n\")\n",
    "    \n",
    "    for col in df.dtypes:\n",
    "        check_column_quality(df, col[0])\n",
    "    print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_quality(df_airlines, \"Airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_quality(df_airports, \"Airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df_quality(df_flights, \"Flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autres operations de filtrage sur le df_airlines pour afficher  des doublons sur des groupes de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airlines \\\n",
    "    .groupby(['Name', 'Code']) \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False) \\\n",
    "    #.show(5,False)\n",
    "\n",
    "\n",
    "df_airlines \\\n",
    "    .select(['ICAO', 'Code', 'Name',]) \\\n",
    "    .where(\"Name == 'Babcock Scandinavian AirAmbulance'\") \\\n",
    "    #.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tache nr 1: La compagnie avec le + de vols en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = extract_flights_toDF()\n",
    "df_flights = df_flights.filter((df_flights.airline_icao != 'N/A') & (df_flights.on_ground == 0))\n",
    "\n",
    "airline = df_flights.groupby(['airline_icao']) \\\n",
    "    .count() \\\n",
    "    .sort('count', ascending=False).first()\n",
    "\n",
    "df_airlines.filter(df_airlines.ICAO == airline['airline_icao']).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tache nr 2: Pour chaque continent, la compagnie avec le + de vols régionaux actifs (continent d'origine == continent de destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights = extract_flights_toDF()\n",
    "df_airports = extract_airports_toDF()\n",
    "df_airlines = extract_airlines_toDF()\n",
    "df_zones = extract_zones_toDF()\n",
    "df_zones.show(5,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
